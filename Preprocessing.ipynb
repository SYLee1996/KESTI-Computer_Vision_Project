{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import datatable as dt\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils import path_list, coordinate_matching, calc_chan_mean, calc_grid_mean, dummy_and_add_feature, feature_encoding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "folder_path = '/data/COMPUTER_VISION/_wizai/'\n",
    "scaler_path = '/data/COMPUTER_VISION/AMSU_PREP/Data/TETE_min-max_scaler.pickle'\n",
    "\n",
    "select_list = [4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "SET_range= {\n",
    "        \"TRAIN\": [\"202106\", \"20210710\"],\n",
    "        \"VALID\": [\"20210711\", \"20210720\"],\n",
    "        \"TEST\": [\"20210721\", \"20210731\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('Data') is False:\n",
    "    os.makedirs('Data')\n",
    "    \n",
    "for save_path in ['/data/COMPUTER_VISION/AMSU_PREP/Data/Normal', '/data/COMPUTER_VISION/AMSU_PREP/Data/Abnormal']:\n",
    "    if os.path.exists(save_path) is False:\n",
    "            os.makedirs(save_path)\n",
    "            \n",
    "not_scale_col = ['nchan', 'sat_id', 'scanpos', 'lat', 'lon', 'REAL_QC', 'grid', 'chan_lat_mean', 'chan_lon_mean', 'grid_lat_mean', 'grid_lon_mean',\n",
    "                        'sin_hour', 'cos_hour', 'sin_day', 'cos_day', 'sin_month', 'cos_month', 'chqcflag_-999', 'chqcflag_0']\n",
    "scale_col = ['bias_pred', 'obsTB', 'innov', 'chan_bias_pred_mean', 'chan_obsTB_mean', 'chan_innov_mean', 'grid_bias_pred_mean', 'grid_obsTB_mean', 'grid_innov_mean']\n",
    "temp_dict = {key:[np.nan,np.nan] for key in scale_col}\n",
    "\n",
    "if not os.path.exists(scaler_path):\n",
    "    print('scaler')\n",
    "    for MODE in ['TRAIN', 'VALID']:\n",
    "        in4bc_path_list, thinn_path_list = path_list(folder_path, SET_range, MODE)\n",
    "\n",
    "        for in4bc_path, InnQC2_path in tqdm(zip(in4bc_path_list, thinn_path_list), total=len(thinn_path_list)):\n",
    "        \n",
    "            # in4bc\n",
    "            in4bc_open_netcdf = xr.open_dataset(in4bc_path)\n",
    "            in4bc_xrdataset = in4bc_open_netcdf.to_dataframe().reset_index()\n",
    "            in4bc_xrdataset = in4bc_xrdataset[in4bc_xrdataset['npredictors']==0][['nchans', 'sat_id', 'scanpos', 'lat', 'lon', 'chqcflag', 'bias_pred', 'obsTB', 'innov']].reset_index(drop=True)\n",
    "            in4bc_xrdataset.rename(columns={'nchans':'nchan'}, inplace=True)\n",
    "            in4bc_xrdataset = in4bc_xrdataset[in4bc_xrdataset['nchan'].isin(select_list)].reset_index(drop=True)\n",
    "\n",
    "            # pre-thinning\n",
    "            pre_thinning_df = dt.fread(InnQC2_path, encoding = \"utf-8\").to_pandas().iloc[:,1:]\n",
    "            pre_thinning_df = pre_thinning_df[pre_thinning_df['irej'] == 0].reset_index(drop=True)\n",
    "            pre_thinning_df['lat'] = pre_thinning_df['lat'].astype(np.float32)\n",
    "            pre_thinning_df['lon'] = pre_thinning_df['lon'].astype(np.float32)\n",
    "            pre_thinning_df.drop(['irej', 'isat', 'bpos', 'QCflag', 'sfctype', 'obstdif', \n",
    "                                'ob(01)', 'ob(02)', 'ob(03)', 'ob(04)', 'ob(05)', 'ob(06)', 'ob(07)', 'ob(08)', 'ob(09)','ob(10)', 'ob(11)', 'ob(12)','ob(13)', 'ob(14)', 'ob(15)',\n",
    "                                'bk(01)', 'bk(02)', 'bk(03)', 'bk(04)', 'bk(05)', 'bk(06)', 'bk(07)', 'bk(08)', 'bk(09)','bk(10)', 'bk(11)', 'bk(12)','bk(13)', 'bk(14)', 'bk(15)',\n",
    "                                'cob(01)', 'cob(02)', 'cob(03)', 'cob(04)', 'cob(05)', 'cob(06)', 'cob(07)', 'cob(08)', 'cob(09)','cob(10)', 'cob(11)', 'cob(12)','cob(13)', 'cob(14)', 'cob(15)',\n",
    "                                'ck(01)', 'ck(02)', 'ck(03)', 'ck(04)', 'ck(15)'], axis=1, inplace=True)\n",
    "            \n",
    "            in4bc_df_labeling = coordinate_matching(in4bc_xrdataset, pre_thinning_df, select_list)\n",
    "            in4bc_df_labeling_class = calc_chan_mean(in4bc_df_labeling, select_list)\n",
    "            in4bc_df_grid = calc_grid_mean(in4bc_df_labeling_class, select_list)   \n",
    "            in4bc_df_grid[['sin_hour', 'cos_hour', 'sin_day', 'cos_day', 'sin_month', 'cos_month']] = dummy_and_add_feature(in4bc_path)\n",
    "            in4bc_df_encoding = feature_encoding(in4bc_df_grid)\n",
    "            \n",
    "            del in4bc_open_netcdf; del in4bc_xrdataset; del pre_thinning_df; del in4bc_df_labeling; del in4bc_df_labeling_class; del in4bc_df_grid;\n",
    "                    \n",
    "            # stn마다 변수별 최대, 최소값 구하기\n",
    "            max_arr = in4bc_df_encoding[scale_col].max().values\n",
    "            min_arr = in4bc_df_encoding[scale_col].min().values\n",
    "                \n",
    "            # 사전에 변수별 최대, 최소값 저장\n",
    "            min_max_dict = {scale_col[i]:[min_arr[i], max_arr[i]] for i in range(len(scale_col))}\n",
    "            \n",
    "            for key in temp_dict.keys():\n",
    "                temp_dict[key][0]=min(min_max_dict[key][0], temp_dict[key][0])\n",
    "                temp_dict[key][1]=max(min_max_dict[key][1], temp_dict[key][1])\n",
    "                \n",
    "    with open(scaler_path, 'wb') as fw:\n",
    "        pickle.dump(temp_dict, fw)\n",
    "    print(\"scaler is saved at {}\".format(scaler_path))\n",
    "                \n",
    "else:\n",
    "    for MODE in ['TRAIN', 'VALID', 'TEST']:\n",
    "        in4bc_path_list, thinn_path_list = path_list(folder_path, SET_range, MODE)\n",
    "\n",
    "        for in4bc_path, InnQC2_path in tqdm(zip(in4bc_path_list, thinn_path_list), total=len(thinn_path_list)):\n",
    "        \n",
    "            # in4bc\n",
    "            in4bc_open_netcdf = xr.open_dataset(in4bc_path)\n",
    "            in4bc_xrdataset = in4bc_open_netcdf.to_dataframe().reset_index()\n",
    "            in4bc_xrdataset = in4bc_xrdataset[in4bc_xrdataset['npredictors']==0][['nchans', 'sat_id', 'scanpos', 'lat', 'lon', 'chqcflag', 'bias_pred', 'obsTB', 'innov']].reset_index(drop=True)\n",
    "            in4bc_xrdataset.rename(columns={'nchans':'nchan'}, inplace=True)\n",
    "            in4bc_xrdataset = in4bc_xrdataset[in4bc_xrdataset['nchan'].isin(select_list)].reset_index(drop=True)\n",
    "\n",
    "            # pre-thinning\n",
    "            pre_thinning_df = dt.fread(InnQC2_path, encoding = \"utf-8\").to_pandas().iloc[:,1:]\n",
    "            pre_thinning_df = pre_thinning_df[pre_thinning_df['irej'] == 0].reset_index(drop=True)\n",
    "            pre_thinning_df['lat'] = pre_thinning_df['lat'].astype(np.float32)\n",
    "            pre_thinning_df['lon'] = pre_thinning_df['lon'].astype(np.float32)\n",
    "            pre_thinning_df.drop(['irej', 'isat', 'bpos', 'QCflag', 'sfctype', 'obstdif', \n",
    "                                'ob(01)', 'ob(02)', 'ob(03)', 'ob(04)', 'ob(05)', 'ob(06)', 'ob(07)', 'ob(08)', 'ob(09)','ob(10)', 'ob(11)', 'ob(12)','ob(13)', 'ob(14)', 'ob(15)',\n",
    "                                'bk(01)', 'bk(02)', 'bk(03)', 'bk(04)', 'bk(05)', 'bk(06)', 'bk(07)', 'bk(08)', 'bk(09)','bk(10)', 'bk(11)', 'bk(12)','bk(13)', 'bk(14)', 'bk(15)',\n",
    "                                'cob(01)', 'cob(02)', 'cob(03)', 'cob(04)', 'cob(05)', 'cob(06)', 'cob(07)', 'cob(08)', 'cob(09)','cob(10)', 'cob(11)', 'cob(12)','cob(13)', 'cob(14)', 'cob(15)',\n",
    "                                'ck(01)', 'ck(02)', 'ck(03)', 'ck(04)', 'ck(15)'], axis=1, inplace=True)\n",
    "            \n",
    "            in4bc_df_labeling = coordinate_matching(in4bc_xrdataset, pre_thinning_df, select_list)\n",
    "            in4bc_df_labeling_class = calc_chan_mean(in4bc_df_labeling, select_list)\n",
    "            in4bc_df_grid = calc_grid_mean(in4bc_df_labeling_class, select_list)   \n",
    "            in4bc_df_grid[['sin_hour', 'cos_hour', 'sin_day', 'cos_day', 'sin_month', 'cos_month']] = dummy_and_add_feature(in4bc_path)\n",
    "            in4bc_df_encoding = feature_encoding(in4bc_df_grid)\n",
    "            \n",
    "            del in4bc_open_netcdf; del in4bc_xrdataset; del pre_thinning_df; del in4bc_df_labeling; del in4bc_df_labeling_class; del in4bc_df_grid;\n",
    "            \n",
    "            with open(scaler_path, 'rb') as fr:\n",
    "                min_max_dict = pickle.load(fr)\n",
    "        \n",
    "            # min-max scaling\n",
    "            for col, (col_min, col_max) in min_max_dict.items():\n",
    "                in4bc_df_encoding[col] = in4bc_df_encoding[col] - col_min\n",
    "                in4bc_df_encoding[col] = in4bc_df_encoding[col] / (col_max-col_min)\n",
    "\n",
    "            in4bc_df_encoding['lat'] = in4bc_df_encoding['lat']/90\n",
    "            in4bc_df_encoding['lon'] = in4bc_df_encoding['lon']/360\n",
    "            in4bc_df_encoding['chan_lat_mean'] = in4bc_df_encoding['chan_lat_mean']/90\n",
    "            in4bc_df_encoding['chan_lon_mean'] = in4bc_df_encoding['chan_lon_mean']/360\n",
    "            in4bc_df_encoding['grid_lat_mean'] = in4bc_df_encoding['grid_lat_mean']/90\n",
    "            in4bc_df_encoding['grid_lon_mean'] = in4bc_df_encoding['grid_lon_mean']/360\n",
    "            \n",
    "            normal_data   = in4bc_df_encoding[in4bc_df_encoding['REAL_QC'] == 0.0].fillna(1).reset_index(drop=True)\n",
    "            abnormal_data = in4bc_df_encoding[in4bc_df_encoding['REAL_QC'] == 7.0].fillna(1).reset_index(drop=True)\n",
    "            \n",
    "            normal_data.drop(['REAL_QC'], axis=1, inplace=True)\n",
    "            abnormal_data.drop(['REAL_QC'], axis=1, inplace=True)\n",
    "            \n",
    "            torch.save(normal_data, \"/data/COMPUTER_VISION/AMSU_PREP/Data/Normal/{}_{}_Normal_data.pkl\".format(in4bc_path.split(\"_\")[-1].split(\".\")[0], MODE), pickle_module=pickle)\n",
    "            torch.save(abnormal_data, \"/data/COMPUTER_VISION/AMSU_PREP/Data/Abnormal/{}_{}_Abnormal_data.pkl\".format(in4bc_path.split(\"_\")[-1].split(\".\")[0], MODE), pickle_module=pickle)\n",
    "            del in4bc_df_encoding; del normal_data; del abnormal_data;\n",
    "            \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
